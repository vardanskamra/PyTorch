{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acdd5149",
   "metadata": {},
   "source": [
    "# Going Modular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42f87ae",
   "metadata": {},
   "source": [
    "Going modular involves turning notebook code (from a Jupyter Notebook or Google Colab notebook) into a series of \n",
    "different Python scripts that offer similar functionality.\n",
    "\n",
    "For example, we could turn our notebook code from a series of cells into the following Python files:\n",
    "- data_setup.py - a file to prepare and download data if needed.\n",
    "- engine.py - a file containing various training functions.\n",
    "- model_builder.py or model.py - a file to create a PyTorch model.\n",
    "- train.py - a file to leverage all other files and train a target PyTorch model.\n",
    "- utils.py - a file dedicated to helpful utility functions.\n",
    "\n",
    "For example, you might be instructed to run code like the following in a terminal/command line to train a model:\n",
    "``` python train.py --model MODEL_NAME --batch_size BATCH_SIZE --lr LEARNING_RATE --num_epochs NUM_EPOCHS ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a95e13",
   "metadata": {},
   "source": [
    "**Directory Structure:**\n",
    "```\n",
    "going_modular/\n",
    "├── going_modular/\n",
    "│   ├── data_setup.py\n",
    "│   ├── engine.py\n",
    "│   ├── model_builder.py\n",
    "│   ├── train.py\n",
    "│   └── utils.py\n",
    "├── models/\n",
    "│   ├── 05_going_modular_cell_mode_tinyvgg_model.pth\n",
    "│   └── 05_going_modular_script_mode_tinyvgg_model.pth\n",
    "└── data/\n",
    "    └── pizza_steak_sushi/\n",
    "        ├── train/\n",
    "        │   ├── pizza/\n",
    "        │   │   ├── image01.jpeg\n",
    "        │   │   └── ...\n",
    "        │   ├── steak/\n",
    "        │   └── sushi/\n",
    "        └── test/\n",
    "            ├── pizza/\n",
    "            ├── steak/\n",
    "            └── sushi/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15f522e",
   "metadata": {},
   "source": [
    "## 1. Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db8441b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "030799d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Didn't find data\\pizza_steak_sushi directory, creating one now...\n",
      "Downloading...\n",
      "Unzipping...\n"
     ]
    }
   ],
   "source": [
    "data_path = Path(\"data/\")\n",
    "image_path = data_path / \"pizza_steak_sushi\"\n",
    "\n",
    "# If image folder doesn't exist, create\n",
    "if image_path.is_dir():\n",
    "    print(f\"{image_path} directory exists...\")\n",
    "else:\n",
    "    print(f\"Didn't find {image_path} directory, creating one now...\")\n",
    "    image_path.mkdir(parents=True, exist_ok=True) \n",
    "        # exist_ok=True  will make the function do nothing if the directory already exists\n",
    "        # parents=True tells Python to create any necessary parent directories\n",
    "    \n",
    "with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
    "    request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
    "    print(\"Downloading...\")\n",
    "    f.write(request.content)\n",
    "    \n",
    "# Unzip\n",
    "with zipfile.ZipFile(data_path/\"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
    "    print(\"Unzipping...\")\n",
    "    zip_ref.extractall(image_path)\n",
    "    \n",
    "# Remove zip file\n",
    "os.remove(data_path/\"pizza_steak_sushi.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e02b46",
   "metadata": {},
   "source": [
    "## 2. Create Datasets and Dataloaders (`data_setup.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c744fdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile going_modular/data_setup.py\n",
    "\n",
    "\"\"\"\n",
    "Contains functionality for creating PyTorch DataLoaders for image classification data.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "\n",
    "def create_dataloaders(train_dir: str,\n",
    "                       test_dir: str,\n",
    "                       transform: transforms.Compose,\n",
    "                       batch_size: int,\n",
    "                       num_workers: int=NUM_WORKERS):\n",
    "    \"\"\"\n",
    "    Creates training and testing DataLoaders\n",
    "    \n",
    "    Args:\n",
    "        train_dir: Path to training directory.\n",
    "        test_dir: Path to testing directory.\n",
    "        transform: torchvision transforms to perform on training and testing data.\n",
    "        batch_size: Number of samples per batch in each of the DataLoaders.\n",
    "        num_workers: An integer for number of workers per DataLoader.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple of (train_dataloader, test_dataloader, class_names).\n",
    "        Where class_names is a list of the target classes.\n",
    "        \n",
    "    Example usage:\n",
    "        train_dataloader, test_dataloader, class_names = create_dataloaders(train_dir=path/to/train_dir,\n",
    "                                                                            test_dir=path/to/test_dir,\n",
    "                                                                            transform=some_transform,\n",
    "                                                                            batch_size=32,\n",
    "                                                                            num_workers=4)\n",
    "    \"\"\"\n",
    "    # Use ImageFolder to create datasets\n",
    "    train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "    test_data = fatasets.ImageFolder(test_dir, transform=transform)\n",
    "    \n",
    "    # Get class names\n",
    "    class_names = train_data.classes\n",
    "    \n",
    "    # Turn images into data loaders\n",
    "    train_dataloader = DataLoader(train_data, \n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=num_workers,\n",
    "                                  pin_memory=True)\n",
    "    test_dataloader = DataLoader(test_data,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=False,\n",
    "                                 num_workers=num_workers,\n",
    "                                 pin_memory=True)\n",
    "    \n",
    "    return train_dataloader, test_dataloader, class_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
